# pytest-llmeval

[![PyPI version](https://img.shields.io/pypi/v/pytest-llmeval.svg)](https://pypi.org/project/pytest-llmeval)
[![Python versions](https://img.shields.io/pypi/pyversions/pytest-llmeval.svg)](https://pypi.org/project/pytest-llmeval)
[![See Build Status on GitHub Actions](https://github.com/kevinschaul/pytest-llmeval/actions/workflows/main.yml/badge.svg)](https://github.com/kevinschaul/pytest-llmeval/actions/workflows/main.yml)

A pytest plugin to evaluate/benchmark LLM prompts

---

This [pytest](https://github.com/pytest-dev/pytest) plugin was generated with [Cookiecutter](https://github.com/audreyr/cookiecutter) along with [@hackebrot](https://github.com/hackebrot)'s [cookiecutter-pytest-plugin](https://github.com/pytest-dev/cookiecutter-pytest-plugin) template.

## Features

* TODO

## Requirements

* TODO

## Installation

You can install "pytest-llmeval" via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):

```
$ pip install pytest-llmeval
```

## Usage

* TODO

## Contributing

Contributions are very welcome. Tests can be run with [tox](https://tox.readthedocs.io/en/latest/), please ensure
the coverage at least stays the same before you submit a pull request.

## License

Distributed under the terms of the [MIT](https://opensource.org/licenses/MIT) license, "pytest-llmeval" is free and open source software

## Issues

If you encounter any problems, please [file an issue](https://github.com/kevinschaul/pytest-llmeval/issues) along with a detailed description.
